/// This enum represents all the possible tokens that can be generated by the lexer
/// The lexer will generate a stream of tokens that will be used by the parser to
/// generate an AST
#[derive(Debug)]
pub enum Token {
    Title,
    Date,
    Description,
    Content,
    Text(String),
    Indent,
    Dedent,
    Paragraph,
    Link(String),
    Image(String),
    EOF,
}

/// The lexer will tokenize the input string and generate a vector of tokens.
/// The lexer will also add indentation and dedentation tokens to the stream
/// which will be used by the parser to generate an AST
pub fn lex(input: &str) -> Vec<Token> {
    let mut tokens = Vec::new();
    let mut indents: Vec<u8> = Vec::new();

    let lines = input.lines();

    for line in lines {
        // skip empty lines
        if line.trim().is_empty() {
            continue;
        }

        // check the indentation level of the line
        let indent_level = line.chars().take_while(|c| c.is_whitespace()).count();

        // check if its a top level line
        if indent_level == 0 {
            // we need to dedent all the previous indentation levels
            for _ in 0..indents.len() {
                tokens.push(Token::Dedent);
            }

            // and clear the stack
            indents.clear();

            // we are at the top level so we can just tokenize the line
            tokens.push(tokenize_line(line));
        } else if indent_level > *indents.last().unwrap_or(&0) as usize {
            // so we are starting a new block with a greater indentation level
            tokens.push(Token::Indent);
            // we need to tokenize the line and add it to the tokens
            tokens.push(tokenize_line(line.trim()));
            // we add the new indentation level to the stack
            indents.push(indent_level as u8);
        } else if indent_level < *indents.last().unwrap_or(&0) as usize {
            // check if the indentation level is less than the last one
            // this means we are ending a block
            let mut dedent_count = 0;

            // check how many dedent tokens we need to add
            // we need to iterate over the stack in reverse order amd break when we find the
            // indentation level that matches the current line
            for indent in indents.iter().rev() {
                if indent_level >= *indent as usize {
                    break;
                }
                dedent_count += 1;
            }

            // add the dedent tokens
            for _ in 0..dedent_count {
                tokens.push(Token::Dedent);
            }

            // remove the dedented indentation levels from the stack
            indents.truncate(indents.len() - dedent_count);

            // we can now tokenize the line
            tokens.push(tokenize_line(line.trim()));
        } else {
            // we are at the same indentation level as the last line
            // The case where this happens would be for multiline text or empty blocks
            // so we can just tokenize the line directly
            tokens.push(tokenize_line(line.trim()));
        }
    }

    // we need to dedent all the previous indentation levels
    for _ in 0..indents.len() {
        tokens.push(Token::Dedent);
    }

    // add a EOF token to the end of the stream
    // tokens.push(Token::EOF);

    tokens
}

/// This function will tokenize a single line into a token
pub fn tokenize_line(token: &str) -> Token {
    match token {
        "title:" => Token::Title,
        "date:" => Token::Date,
        "description:" => Token::Description,
        "content:" => Token::Content,
        "p:" => Token::Paragraph,
        _ if token.starts_with("link:") => Token::Link(token[5..].trim().to_string()),
        _ if token.starts_with("img:") => Token::Image(token[4..].trim().to_string()),
        txt => Token::Text(txt.trim().to_string()),
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    /// This function will print the tokens to the console with the correct indentation
    ///
    /// This is only used for testing
    fn print_tokens(tokens: Vec<Token>) {
        let mut indent_str = String::new();

        for token in tokens {
            match token {
                Token::Indent => indent_str.push_str("    "),
                Token::Dedent => {
                    indent_str.pop();
                    indent_str.pop();
                    indent_str.pop();
                    indent_str.pop();
                }
                _ => println!("{}{:?}", indent_str, token),
            }
        }
    }

    #[test]
    fn test_tokenize() {
        let input = include_str!("../examples/hello_world/hello_world.kml");
        let tokens = lex(input);
        println!("Tokens: {:#?}", tokens);
        print_tokens(tokens);
    }
}
